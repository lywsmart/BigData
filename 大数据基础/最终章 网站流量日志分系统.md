# 最终章 综合项目—网站流量日志分系统

​	本章将通过Hadoop生态体系技术实现[网站流量日志分析系统]()，来帮助读者学习大数据体系架构的开发流程，以及利用现有技术解决实际生活中遇到的问题。

​	本章的核心是在[掌握网站流量日志数据分析系统]()的业务流程的前提下，具备独立[分析日志数据]()，并利用[MapReduce]()技术将数据[提取]()出易于分析的数据结构，以及使用[Hive完成数据分析]()，计算出相关需求结果的能力。



## 9.1 系统概述

​	了解系统概述，能够说出本系统的[背景]()、[需求]()和[架构]()

### 9.1.1 系统背景介绍

​	近年来，随着社会的不断发展，人们对于[海量数据]()的[挖掘]()和[运用]()越来越重视，大数据的统计分析可以为企业决策者提供充实的依据。

​	例如，通过对某[网站日志数据统计分析]()，可以得出网站的日访问量，从而得出网站的受欢迎程度；通过对移动APP的下载数据量进行统计分析，可得出应用程序的受欢迎程度，可以通过不同维度进行更深层次的数据分析，为运营分析与推广决策提供可靠的[数据依据]()。



### 9.1.2 需求分析

​	在大数据开发中，通常首要任务是[明确分析目的]()，使开发人员能准确地根据具体的需求去过滤数据并通过大数据技术进行数据分析和处理，将处理结果以图表等可视化的形式展示出来。

​	本项目分析的数据主要是用户在2022年8月份访问网站时产生的日志数据，具体分为以下两点需求。

#### (1) 浏览量（PV）分析

​	[浏览量分析]()是指统计网站的页面被用户[浏览的次数]()，它是[衡量网站质量]()的重要指标。在日志数据中，每一条数据就代表了网站的页面被浏览一次。

​	本项目统计网站在2022年8月份每天的浏览量。

#### (2) 人均浏览页数分析

​	[人均浏览页面分析]()是指统计网站的页面被[每个用户平均浏览的次数]()，它可以反应出网站对用户的粘性程度。

​	[人均浏览页面分析]()是通过浏览量和用户数（[UV]()）这两个指标计算得出，计算公式为[浏览量/用户数]()。





### 9.1.3 系统架构

<img src="./最终章 网站流量日志分系统.assets/image-20231214162551573.png" alt="image-20231214162551573" />



​	根据网站流量日志数据分析系统的整体流程，可以将该系统的实现分为[数据采集]()、[数据预处理]()、[数据仓库开发]()、[数据分析]()、[数据导出]()和[数据可视化]()这6个步骤。



#### 第一步：数据采集

​	在网站流量日志数据分析系统中，数据采集是将[网站产生的日志数据]()通过[Flume采集到HDFS]()，这样做的目的是能够避免因网站产生的日志数据过多，影响服务器自身存储的问题。



#### 第二步：数据预处理

​	数据预处理是指在采集的日志数据被分析之前，利用[MapReduce程序对这些数据进行清洗]()，例如根据实际业务场景提取日志数据中的部分内容、对数据进行规范化处理等。



#### 第三步：数据仓库开发

​	数据预处理完后的[半结构化数据]()通常会加载到[Hive数据仓库]()中，然后创建相应的数据库和表与预处理后的结构化数据进行映射关联，这样后续就可以使用HiveQL语句对数据进行分析。



#### 第四步：数据分析

​	[数据分析]()是网站流量日志数据分析系统中的核心内容，即根据相关需求使用[HiveQL语句]()，对指定表中的数据[进行分析]()，得出各种[指标结果]()。



#### 第五步：数据导出

​	数据分析完成后得到的指标结果存在[Hive的表]()中，为了方便后续进行[数据可视化处理]()，一般需要将[Hive表中的数据导出到MySQL的数据表]()。



#### 第六步：数据可视化

​	数据可视化是利用[BI]()工具获取[MySQL]()中数据表的数据，并将数据以[图表]()的形式展现出来，这样能够直观、简洁的提供给网站管理者和运营者进行决策与分析。





## 9.2 准备工作 模拟数据来源

​	本项目主要通过编写Java程序[模拟Nginx服务生成日志数据]()，并将生成的日志数据写入到指定的日志文件中供Flume采集。

​	（[这一部分不用学习，直接用下发的Jar包！！！]()）



### 9.2.1 创建Maven项目

​	在IDEA创建一个名为[LogDataSource]()的Maven项目，创建包[com.lcvc.generate]()。

<img src="./最终章 网站流量日志分系统.assets/image-20231214170428829.png" alt="image-20231214170428829" style="zoom:67%;" />





### 9.2.2 实现Java程序

​	在包com.lcvc.generate下创建类[GenerateLog]()，[编写模拟Nginx服务生成日志数据的代码]()。

```java
package com.lcvc.generate;


import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.lang.reflect.Array;
import java.text.DateFormat;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.Locale;
import java.util.Random;
import java.util.concurrent.TimeUnit;

public class GenerateLog {

    // 获取想要得到的所有预定义的url地址。
    public static String[] url_paths = {
            "article/112.html",
            "article/113.html",
            "article/114.html",
            "article/115.html",
            "article/116.html",
            "article/117.html",
            "article/118.html",
            "article/119.html",
            "video/821",
            "tag/list"
    };


    // 生成IP地址
    public static String[] ip_splices =
            {"102","71","145","33","67","54","164","121"
            };


    // 生成状态码
    public static String[] status_codes = {"200","404","500"};


    // 随机生成IP地址
    public static String randomIp(){
        int ipNum;
        StringBuilder ip = new StringBuilder();
        for (int i=0; i<4; i++){
            ipNum = new Random().nextInt(Array.getLength(ip_splices));
            ip.append(".").append(ip_splices[ipNum]);
        }
        return ip.substring(1);
    }

    // 随机生成状态码
    public static String randomCode(){
        int codeNum = new Random().nextInt(Array.getLength(status_codes));
        return status_codes[codeNum];
    }

    // 随机生成
    public static String randomUrl(){
        int urlNum = new Random().nextInt(Array.getLength(url_paths));
        return url_paths[urlNum];
    }

    // 随机生成日期
    public static String getDate() throws ParseException {
        Random random = new Random();
        String date;
        DateFormat dateFormat =
                new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss", Locale.ENGLISH);
        String days = String.valueOf(random.nextInt(30)+1);
        String hour = String.valueOf(random.nextInt(24));
        String minute = String.valueOf(random.nextInt(60));
        String second = String.valueOf(random.nextInt(60));
        date = days + "/"
                + "Aug" + "/"
                + "2022" + ":"
                + hour + ":"
                + minute + ":"
                + second;
        Date parseDate = dateFormat.parse(date);
        return dateFormat.format(parseDate);
    }




    private static String generateLog() throws ParseException {
        return randomIp()
                + "|"

                + "["
                + getDate()
                + "]"
                +"|"

                + "\"GET /"
                + randomUrl()
                + " HTTP/1.1\""

                + "|"
                + randomCode();

    }



    // 运行jar文件时，通过参数指定日志数据的输出目录，间隔5秒钟生成1条日志数据。
    public static void main(String[] args)
            throws InterruptedException, IOException, ParseException {

        String path = args[0];

        File file = new File(path);

        while (true){
            FileOutputStream fos = new FileOutputStream(file, true);
            String content = generateLog()+"\n";
            fos.write(content.getBytes());
            TimeUnit.SECONDS.sleep(5);
            fos.close();
        }


    }

}

```

​	

​	通过Idea进行打包（步骤略）

​	最终生成Jar包

<img src="./最终章 网站流量日志分系统.assets/image-20231214173947926.png" alt="image-20231214173947926" style="zoom:75%;" />





### 9.2.3 上传Jar包

​	将Jar上传到master的/opt/software文件夹

<img src="./最终章 网站流量日志分系统.assets/image-20231214174210560.png" alt="image-20231214174210560" />



### 9.2.4 创建日志目录

​	在虚拟机master的/opt/software/目录下新建/data/weblog目录，[用于存放日志文件](/data/weblog)，具体命令如下。

```shell
mkdir -p /opt/software/data/weblog
```

<img src="./最终章 网站流量日志分系统.assets/image-20231214174403225.png" alt="image-20231214174403225" />



### 9.2.5 测试JAR

​	运行[LogDataSource.jar]()文件。

​	并指定日志数据写入到[/opt/software/data/weblog](/opt/software/data/weblog)目录下的[nginx.log](/opt/software/data/weblog)文件中。

​	执行命令

```shell
java -cp /opt/software/LogDataSource.jar \
com.lcvc.generate.GenerateLog \
/opt/software/data/weblog/nginx.log
```

<img src="./最终章 网站流量日志分系统.assets/image-20231214174847367.png" alt="image-20231214174847367" style="zoom:67%;" />





### 9.2.6 查看日志数据

​	开启一个新的虚拟机master窗口，

​	执行如下命令查看日志文件nginx.log中实时写入的日志数据。

```shell
tail -f /opt/software/data/weblog/nginx.log
```

<img src="./最终章 网站流量日志分系统.assets/image-20231218193759035.png" alt="image-20231218193759035" style="zoom:67%;" />





### 再学一招：用Shell脚本后台运行JAR任务

​	如果必须要开启会话来保持模拟任务，会非常不方便，所以我们可以用[Shell脚本]()在后台运行模拟数据的任务。

​	创建一个存放项目脚本的目录

```shell
mkdir -p /opt/software/bin
```

#### (1)编写Shell脚本

```shell
vi /opt/software/bin/startlog.sh
```

​	编写以下脚本

```shell
#!/bin/bash
# sh startlog.sh stop	关闭模拟程序
# sh startlog.sh start	启动模拟程序

# 设置LogDataSource.jar文件的路径
JAR_FILE="/opt/software/LogDataSource.jar"

# 设置日志文件路径
LOG_FILE="/opt/software/bin/startlog.txt"

# 设置保存进程ID的文件路径
PID_FILE="/opt/software/bin/pid.txt"

start() {
  if [ -f "$PID_FILE" ]; then
    echo "LogDataSource.jar 已经在运行，进程ID：$(cat $PID_FILE)"
  else
    nohup java -cp "$JAR_FILE" com.lcvc.generate.GenerateLog /opt/software/data/weblog/nginx.log >> "$LOG_FILE" 2>&1 &
    echo $! > "$PID_FILE"
    echo "LogDataSource.jar 正在后台运行，进程ID保存在 $PID_FILE 中。"
  fi
}

stop() {
  if [ -f "$PID_FILE" ]; then
    PID=$(cat "$PID_FILE")
    kill $PID
    rm "$PID_FILE"
    echo "LogDataSource.jar 已经停止，进程ID：$PID"
  else
    echo "LogDataSource.jar 没有在运行。"
  fi
}

case "$1" in
  start)
    start
    ;;
  stop)
    stop
    ;;
  restart)
    stop
    start
    ;;
  *)
    echo "用法: $0 {start|stop|restart}"
    exit 1
    ;;
esac

exit 0

```

#### (2)授予脚本文件[可执行权限]()

```shell
chmod +x /opt/software/bin/startlog.sh
```

#### (3)启动脚本

```shell
/opt/software/bin/startlog.sh start
```

<img src="./最终章 网站流量日志分系统.assets/image-20231214190430871.png" alt="image-20231214190430871" style="zoom:50%;" />



​	通过JPS查看该进程

<img src="./最终章 网站流量日志分系统.assets/image-20231214190508772.png" alt="image-20231214190508772" style="zoom:67%;" />



## 9.3 模块一 数据采集

​	掌握数据采集，能够独立完成[Flume采集数据]()的操作

### 9.3.1 编写Flume采集方案

​	使用[Flume采集]()日志文件nginx.log中的日志数据，将采集的日志数据[写入到HDFS]()。

​	在虚拟机master中创建文件WebLog.conf，在该文件中指定采集方案。

```shell
vi $FLUME_HOME/job/WebLog.conf
```

​	编写采集方案

```python
a1.sources = r1
a1.sinks = k1
a1.channels = c1
# 指定source类型为Exec，用于监控文件nginx.log内数据的变化
a1.sources.r1.type = exec
a1.sources.r1.command = tail -f /opt/software/data/weblog/nginx.log
# 指定sink类型为HDFS Sink，将实时采集的日志数据写入HDFS的指定目录
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /weblog/%y-%m-%d/%H-%M/
a1.sinks.k1.hdfs.filePrefix = lcvc-
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.useLocalTimeStamp = true
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```



### 9.3.2 Flume采集数据

​	接下来，[确保LogDataSource.jar文件处于运行状态]()下，根据指定的采集方案，在虚拟机master启动Agent，采集日志文件nginx.log中的日志数据。

```shell
flume-ng agent \
--name a1 \
--conf $FLUME_HOME/job/ \
--conf-file $FLUME_HOME/job/WebLog.conf \
 -Dflume.root.logger=INFO,console
```



<img src="./最终章 网站流量日志分系统.assets/image-20231214191136053.png" alt="image-20231214191136053" />



​	通过HDFS的Web UI查看[/weblog/](/weblog/)目录中的内容。

​	([注意：时间是实时的]())

<img src="./最终章 网站流量日志分系统.assets/image-20231214191413438.png" alt="image-20231214191413438" />

​	



### 再学一招：用Shell脚本后台运行Flume任务

​	如果必须要开启会话来保持模拟任务，会非常不方便，所以我们可以用[Shell脚本]()在后台运行Flume采集数据的任务。

#### (1)编写Shell脚本

```shell
vi /opt/software/bin/startflume.sh
```

​	编写以下脚本

```shell
#!/bin/bash
# sh startflume.sh stop	关闭模拟程序
# sh startflume.sh start 启动模拟程序

# 设置Flume的根目录
export FLUME_HOME=/usr/local/src/flume

# Flume任务命令
FLUME_COMMAND="flume-ng agent \
  --name a1 \
  --conf $FLUME_HOME/job/ \
  --conf-file $FLUME_HOME/job/WebLog.conf \
  -Dflume.root.logger=INFO,console"

# 设置日志文件路径
LOG_FILE="/opt/software/bin/flume.log"

# 设置保存进程ID的文件路径
PID_FILE="/opt/software/bin/flume.pid"

start() {
  if [ -f "$PID_FILE" ]; then
    echo "Flume任务已经在运行，进程ID：$(cat $PID_FILE)"
  else
    nohup $FLUME_COMMAND >> "$LOG_FILE" 2>&1 &
    echo $! > "$PID_FILE"
    echo "Flume任务正在后台运行，进程ID保存在 $PID_FILE 中。"
  fi
}

stop() {
  if [ -f "$PID_FILE" ]; then
    PID=$(cat "$PID_FILE")
    kill $PID
    rm "$PID_FILE"
    echo "Flume任务已经停止，进程ID：$PID"
  else
    echo "Flume任务没有在运行。"
  fi
}

case "$1" in
  start)
    start
    ;;
  stop)
    stop
    ;;
  restart)
    stop
    start
    ;;
  *)
    echo "用法: $0 {start|stop|restart}"
    exit 1
    ;;
esac

exit 0

```

#### (2)授予脚本文件[可执行权限]()

```shell
chmod +x /opt/software/bin/startflume.sh
```

#### (3)启动脚本

```shell
/opt/software/bin/startflume.sh start
```



​	通过ps查看该进程

```shell
ps aux | grep flume
```

<img src="./最终章 网站流量日志分系统.assets/image-20231214192529656.png" alt="image-20231214192529656" style="zoom:67%;" />





## 9.4 模块二 数据预处理

​	从日志文件收集的日志数据，通常情况下，是[不能直接进行数据分析]()的。

​	因为日志数据的[格式比较复杂]()，不便于对这些数据进行数据分析的处理。

​	因此对日志数据进行数据分析之前，需要对采集的日志数据进行[数据预处理]()的操作。

​	我们要把数据处理成以下格式：

```
102.71.67.145,2022-08-26 12:33:06,GET,/article/118.html,200
102.71.67.71,2022-08-14 16:58:44,GET,/article/119.html,200
121.102.164.67,2022-08-07 21:14:59,GET,/article/113.html,200
121.121.102.102,2022-08-12 03:51:41,GET,/tag/list,200
121.121.33.33,2022-08-27 21:04:07,GET,/article/112.html,200
121.121.67.102,2022-08-25 23:14:38,GET,/article/117.html,200
```

### 9.4.1 任务分析

#### (1) 字段分析

​	在本项目中，通过编写[MapReduce程序]()对采集的数据进行数据预处理操作。

提取每条日志数据中：

1. **IP地址**

2. **用户浏览网站时的日期和时间**，并且对用户浏览网站时的日期和时间进行标准化处理

   将默认的日期和时间格式02/Aug/2022:14:51:54转换为2022-08-02 14:51:54。

3. **请求方式**

4. **用户浏览网站的URL**

5. **响应状态码**



#### (2) MR分析

​	数据预处理的MapReduce程序仅仅是对每条日志数据进行[提取]()和[转换]()的处理，并且将每条日志数据处理完的结果直接输出即可。

​	不需要对多条日志数据的处理结果进行[聚合操作]()，编写的MapReduce程序[仅需要实现Map阶段]()即可，有关实现数据预处理的操作步骤如下。

​	

### 9.4.2 创建Maven项目

​	在IDEA创建一个名为[HadoopDataReport]()的Maven项目，在该项目中创建包[com.lcvc.mr]()，该包用于存放实现MapReduce程序的相关类。

<img src="./最终章 网站流量日志分系统.assets/image-20231214195031407.png" alt="image-20231214195031407" />

### 9.4.3 添加项目依赖

​	在项目HadoopDataReport的[pom.xml]()文件中添加[相关依赖]()。

```xml
  <properties>
    <hadoop.version>2.7.1</hadoop.version>
  </properties>

  <dependencies>
    <dependency>
     	<groupId>org.apache.hadoop</groupId>
     	<artifactId>hadoop-client</artifactId>
     	<version>${hadoop.version}</version>
    </dependency>
      
    <dependency>
    	<groupId>org.apache.hadoop</groupId>
    	<artifactId>hadoop-hdfs</artifactId>
    	<version>${hadoop.version}</version>
	</dependency>

  </dependencies>
```

<img src="./最终章 网站流量日志分系统.assets/image-20231214195205659.png" alt="image-20231214195205659" style="zoom: 50%;" />





### 9.4.4 实现MR程序(按步骤)

#### (1) Map类

​	实现Map阶段需要用户自定义Mapper组件，在HadoopDataReport项目的com.lcvc.mr包中创建用于自定义Mapper组件的类[WebLogMap]()，在该类中编写[数据预处理的相关代码]()。

1. 创建[WebLogMap]()类，并重写[map方法()]()，创建[dateFotmatLog对象]()和[dateFormat对象]()。

   ```java
   public class WebLogMap extends Mapper<LongWritable, Text, Text, NullWritable> {
       @Override
       protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, NullWritable>.Context context) throws IOException, InterruptedException {
           
           // 创建dateFotmatLog对象和dateFormat对象
           DateFormat dateFormatLog = new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss", Locale.ENGLISH);
           
           DateFormat dateFormat =  new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
       }
   }
   ```

   <img src="./最终章 网站流量日志分系统.assets/image-20231215112056169.png" alt="image-20231215112056169" style="zoom:67%;" />

2. 获取[logData]() 、[splitLogData]() 、[ip]()、[date]()、[request]()、[url]()和[code]()。

   ```java
           // 获取logData 、splitLogData 、ip、date、request、url和code。
           String logData = value.toString();
           // 根据 | 切割行
           String[] splitLogData = logData.split("\\|");
           // 获取ip
           String ip = splitLogData[0];
           // 获取请求并截取"GET /article/113.html HTTP/1.1" --> GET
   
           String request =splitLogData[2].substring(1,splitLogData[2].indexOf(" "));
   
           // 获取请求并截取url "GET /article/113.html HTTP/1.1" -->
           String url = splitLogData[2].substring(
                   splitLogData[2].indexOf(" "),splitLogData[2].lastIndexOf(" ")).trim();
           // 获取状态码
           String code = splitLogData[3];
   
   ```

   <img src="./最终章 网站流量日志分系统.assets/image-20231218201027074.png" alt="image-20231218201027074" />

3. 从每条日志数据中提取用户浏览网站时的日期和时间，并且对日期和时间进行标准化处理，[将日期和时间格式转化为指定格式]()。

   ```java
           // 处理日期
           String date="";
   
           // 将日期和时间格式转化为指定格式
           try {
               date = dateFormat.format(
                       // 将28/Aug/2022:22:59:45 转换成标准时间格式  2022-8-28 22:59:45
                       dateFormatLog.parse(
                               // 截取[28/Aug/2022:22:59:45] -->28/Aug/2022:22:59:45
                               splitLogData[1].substring
                                       (1, splitLogData[1].length()-1
                                       )
                       )
               );
           } catch (ParseException e) {
               e.printStackTrace();
           }
   ```

   <img src="./最终章 网站流量日志分系统.assets/image-20231218201753170.png" alt="image-20231218201753170" />

4. 通过字符`,`拼接IP地址、用户浏览网站时的日期和时间、请求方式、用户浏览网站的URL、响应状态码。

   ```java
           String message = ip + "," + date + "," + request + "," + url + "," + code;
   
           context.write(new Text(message),NullWritable.get());
   
   ```
   
   <img src="./最终章 网站流量日志分系统.assets/image-20231218194516507.png" alt="image-20231218194516507" />



#### (2) 驱动类

​	在HadoopDataReport项目的com.itcast.hadoop包中创建MapReduce程序的驱动类[WebLogDriver]()，在该类中编写配置MapReduce程序的相关代码。

1. 创建[WebLogDriver]()类，并定义main()方法，创建dateFormat对象，并获取date对象

   ```java
   public class WebLogDriver {
       public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
           DateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd-HH-mm-ss");
           String date = dateFormat.format(new Date());
           // HDFS设置
           Configuration configuration = new Configuration();
           //指定MR可以在远程集群运行
           configuration.set("mapreduce.app-submission.cross-platform", "true");
           //指定yarn resourceManager的位置
           configuration.set("yarn.resourcemanager.hostname", "master");
       }
   }
   ```

   <img src="./最终章 网站流量日志分系统.assets/image-20231218204142980.png" alt="image-20231218204142980" style="zoom:67%;" />

2. 配置MapReduce程序可以[递归处理]()指定目录及子目录中的所有文件。

   ```java
           // 配置MapReduce程序可以递归处理指定目录及子目录中的所有文件
           configuration.set("mapreduce.input.fileinputformat.input.dir.recursive","true");
   ```

   <img src="./最终章 网站流量日志分系统.assets/image-20231215113758227.png" alt="image-20231215113758227" />

3. 获取job对象，合并小文件。

   ```java
      Job job = Job.getInstance(configuration);
           // import org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;
           job.setInputFormatClass(CombineTextInputFormat.class);
   ```

   <img src="./最终章 网站流量日志分系统.assets/image-20231215113809521.png" alt="image-20231215113809521" />

4. 关联Map程序

   ```java
           // 关联本Driver程序的Jar
           job.setJarByClass(WebLogDriver.class);
   
           // 关联Mapper的Jar和Reduce的Jar
           job.setMapperClass(WebLogMap.class);
   
           // 设置Mapper的输出的key和Value的类型
           job.setMapOutputKeyClass(Text.class);
           job.setMapOutputValueClass(NullWritable.class);
   ```

   <img src="./最终章 网站流量日志分系统.assets/image-20231218201857470.png" alt="image-20231218201857470" />

5. 指定文件的所在目录，并指定MapReduce程序处理结果的输出目录。

   ```java
           // 指定文件的所在目录，并指定MapReduce程序处理结果的输出目录。
           // import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
           // import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
           FileInputFormat.setInputPaths(job,new Path("/weblog"));
           FileOutputFormat.setOutputPath(job,new Path("/output/weblog/"+date));
           boolean result = job.waitForCompletion(true);
           System.exit(result ? 0 : 1);
   ```



### 9.4.5 实现MR程序(完整代码)

#### (1) Map类完整代码

```java
package com.lcvc.mr;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;
import java.text.DateFormat;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Locale;

public class WebLogMap extends Mapper<LongWritable, Text, Text, NullWritable> {
    @Override
    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, NullWritable>.Context context) throws IOException, InterruptedException {
        // 创建dateFotmatLog对象和dateFormat对象
        DateFormat dateFormatLog = new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss", Locale.ENGLISH);
        DateFormat dateFormat =  new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");

        // 获取logData 、splitLogData 、ip、date、request、url和code。
        String logData = value.toString();
        // 根据 | 切割行
        String[] splitLogData = logData.split("\\|");
        // 获取ip
        String ip = splitLogData[0];
        // 获取请求并截取"GET /article/113.html HTTP/1.1" --> GET

        String request =splitLogData[2].substring(1,splitLogData[2].indexOf(" "));

        // 获取请求并截取url "GET /article/113.html HTTP/1.1" -->
        String url = splitLogData[2].substring(
                splitLogData[2].indexOf(" "),splitLogData[2].lastIndexOf(" ")).trim();

        // 获取状态码
        String code = splitLogData[3];

        // 处理日期
        String date="";

        // 将日期和时间格式转化为指定格式
        try {
            date = dateFormat.format(
                    // 将28/Aug/2022:22:59:45 转换成标准时间格式  2022-8-28 22:59:45
                    dateFormatLog.parse(
                            // 截取[28/Aug/2022:22:59:45] -->28/Aug/2022:22:59:45
                            splitLogData[1].substring
                                    (1, splitLogData[1].length()-1
                                    )
                    )
            );
        } catch (ParseException e) {
            e.printStackTrace();
        }

        String message = ip + "," + date + "," + request + "," + url + "," + code;

        System.out.println(message);

        context.write(new Text(message),NullWritable.get());


    }
}

```



#### (2) 驱动类完整代码

```java
package com.lcvc.mr;


import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;
import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.Date;

public class WebLogDriver {
    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
        DateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd-HH-mm-ss");
        String date = dateFormat.format(new Date());
        // HDFS设置
        Configuration configuration = new Configuration();
        // 设置集群模式
        configuration.set("mapreduce.framework.name","yarn");
        //指定MR可以在远程集群运行
        configuration.set("mapreduce.app-submission.cross-platform", "true");
        //指定yarn resourceManager的位置
        configuration.set("yarn.resourcemanager.hostname", "master");

        // 配置MapReduce程序可以递归处理指定目录及子目录中的所有文件
        configuration.set("mapreduce.input.fileinputformat.input.dir.recursive","true");

        // 合并小文件
        Job job = Job.getInstance(configuration);
        // import org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;
        job.setInputFormatClass(CombineTextInputFormat.class);

        // 关联本Driver程序的Jar
        job.setJarByClass(WebLogDriver.class);

        // 关联Mapper的Jar和Reduce的Jar
        job.setMapperClass(WebLogMap.class);

        // 设置Mapper的输出的key和Value的类型
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(NullWritable.class);

        // 指定文件的所在目录，并指定MapReduce程序处理结果的输出目录。
        // import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
        // import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
        FileInputFormat.setInputPaths(job,new Path("/weblog"));
        FileOutputFormat.setOutputPath(job,new Path("/output/weblog/" + date));
        boolean result = job.waitForCompletion(true);
        System.exit(result ? 0 : 1);

    }
}

```



### 9.4.6 打JAR包

1. 选择**【File】---> 【Project Structure】**

   <img src="./最终章 网站流量日志分系统.assets/image-20231215115342041.png" alt="image-20231215115342041" style="zoom: 80%;" />

   

2. 选择**【Artifacts】，并点击【+】**

   <img src="./最终章 网站流量日志分系统.assets/image-20231215115559021.png" alt="image-20231215115559021" style="zoom: 67%;" />

   

3. 选择**【JAR】---> 【From modules with dependencies】**  

   <img src="./最终章 网站流量日志分系统.assets/image-20231215115817482.png" alt="image-20231215115817482" style="zoom:67%;" />

   PS：这里选择【JAR】---> 【Empty】也是，如果选择【Empty】就不会将相关依赖打包，我们可以直接去用虚拟机上的相关依赖（在[$HADOOP_HOME/lib]()上）

   

4. 选择驱动类作为【Main Class】，并点击【OK】

   <img src="./最终章 网站流量日志分系统.assets/image-20231215120221006.png" alt="image-20231215120221006" style="zoom:67%;" />

   点击后，就能发现，我们新增了该JAR包的Artifacts

   <img src="./最终章 网站流量日志分系统.assets/image-20231215120507220.png" alt="image-20231215120507220" style="zoom: 50%;" />

   最后点击右下角的【OK】

   

5. 选择**【Build】 --->  【Build Artifacts】**

   <img src="./最终章 网站流量日志分系统.assets/image-20231215120723794.png" alt="image-20231215120723794" style="zoom:80%;" />

   

6. 在弹出的窗口选择【bulid】

   <img src="./最终章 网站流量日志分系统.assets/image-20231215120902404.png" alt="image-20231215120902404" style="zoom:67%;" />

   最后就能在[out/artifacts/HadoopDataReport_jar](out/artifacts/HadoopDataReport_jar)目录下看到生成的JAR包了。

   <img src="./最终章 网站流量日志分系统.assets/image-20231215121028744.png" alt="image-20231215121028744" />

### 9.4.7 上传JAR包

​	将[HadoopDataReport.jar](out/artifacts/HadoopDataReport_jar/HadoopDataReport.jar)上传到虚拟机master的[/opt/software](/opt/software)目录中

<img src="./最终章 网站流量日志分系统.assets/image-20231215121317570.png" alt="image-20231215121317570" />

### 9.4.8 测试JAR

​	确保Hadoop集群处于启动状态下运行[HadoopDataReport.jar]()。并且需要暂时[停止Flume收集]()。

<img src="./最终章 网站流量日志分系统.assets/image-20231219100244046.png" alt="image-20231219100244046" style="zoom: 67%;" />

​	在虚拟机master执行如下命令。

```shell
hadoop jar /opt/software/HadoopDataReport.jar com.lcvc.mr.WebLogDriver
```

<img src="./最终章 网站流量日志分系统.assets/image-20231215121604756.png" alt="image-20231215121604756" style="zoom:80%;" />

​	运行结束后，我们来到Web UI查看生成的文件

​	[注意：文件的名字是运行的时间，每个人都不一样]()

<img src="./最终章 网站流量日志分系统.assets/image-20231218185105421.png" alt="image-20231218185105421" />

​	查看该文件发现（自己写命令），数据预处理已经完成。

<img src="./最终章 网站流量日志分系统.assets/image-20231218202123639.png" alt="image-20231218202123639" style="zoom:80%;" />





## 9.5 模块三 数据仓库开发

​	掌握数据仓库开发，能够独立完成[Hive数据仓库的构建]()



### 9.5.1 数据仓库设计

​	根据数据仓库设计思路：

<img src="./最终章 网站流量日志分系统.assets/image-20231218204700431.png" alt="image-20231218204700431" style="zoom: 50%;" />

​	在Hive数据仓库中创建[数据仓库层]()对应的

1. 明细表[weblog_dwd]()
2. 中间表[weblog_dwm]()
3. 业务表[weblog_pv_da]()
4. 业务表[weblog_avguv_da]()



#### (1) 明细表weblog_dwd

​	[明细表weblog_dwd]()存在于数据仓库层的[明细层]()，作用是存储通过MapReduce程序进行数据预处理之后的数据。

|   **字段**    | **数据类型** |          **描述**          |
| :-----------: | :----------: | :------------------------: |
|   visit_ip    |    STRING    |   用户访问网站时的IP地址   |
|  visit_date   |    STRING    | 用户访问网站时的日期和时间 |
| visit_request |    STRING    |  用户访问网站时的请求方式  |
|   visit_url   |    STRING    |   用户访问网站的URL地址    |
|  visit_code   |    STRING    |  用户访问网站的响应状态码  |



#### (2) 中间表weblog_dwm

​	明细表[weblog_dwd]()中字段visit_date存储的数据包含用户访问网站时的[日期和时间]()，在这里需要对明细表[weblog_dwd]()的数据进行处理，处理结果[存储]()在[中间表weblog_dwm]()。

|       **字段**       | **数据类型** |         **描述**         |
| :------------------: | :----------: | :----------------------: |
|       visit_ip       |    STRING    |  用户访问网站时的IP地址  |
| [visit_split_date]() |    STRING    |   用户访问网站时的日期   |
| [visit_split_time]() |    STRING    |   用户访问网站时的时间   |
|    visit_request     |    STRING    | 用户访问网站时的请求方式 |
|      visit_url       |    STRING    |  用户访问网站的URL地址   |
|      visit_code      |    STRING    | 用户访问网站的响应状态码 |



#### (3)业务表weblog_pv_da

​	[业务表weblog_pv_da]()存在于数据仓库层的业务(应用)层，作用是[存储浏览量分析的结果]()。

| **字段** | **数据类型** | **描述** |
| :------: | :----------: | :------: |
| datestr  |    STRING    |   日期   |
|    pv    |    BIGINT    |  浏览量  |



#### (4)业务表weblog_avguv_da

​	业务表[weblog_avguv_da]()存在于数据仓库层的业务(应用)层，作用是[存储人均浏览页分析的结果]()。

| **字段** | **数据类型** |      **描述**      |
| :------: | :----------: | :----------------: |
| datestr  |    STRING    |        日期        |
|  avguv   |     INT      | 人均浏览页面的数量 |





### 9.5.2 连接Hive

#### 方式一：直接启动

​	直接在master上打开Hive

<img src="./最终章 网站流量日志分系统.assets/image-20231218210328504.png" alt="image-20231218210328504" style="zoom:80%;" />





#### 方式二：远程连接

​	（[只适用于做过Hive远程连接的同学]()）

​	我们在虚拟机master启动MetaStore和HiveServer2服务。

```
[root@master ~]# hiveserver2
```

<img src="./最终章 网站流量日志分系统.assets/image-20231218210511048.png" alt="image-20231218210511048" style="zoom:80%;" />

​	

​	在虚拟机slave1使用Hive提供的客户端工具Beeline连接HiveServer2服务实现数据仓库的构建。

```
[root@slave1 ~]# beeline -u jdbc:hive2://master:10000 –n root
```

<img src="./最终章 网站流量日志分系统.assets/image-20231218211020736.png" alt="image-20231218211020736" style="zoom: 67%;" />



### 9.5.3 创建数据仓库

​	在Hive中[创建数据库weblog_dw]()，该数据库用于存储数据仓库层的表，命令如下。

```hive
create database if not exists weblog_dw location '/weblog_dw';
```

<img src="./最终章 网站流量日志分系统.assets/image-20231219095302968.png" alt="image-20231219095302968" />

​	



### 9.5.4 实现数据仓库

​	在数据库[weblog_dw]()中创建明细表[weblog_dwd]()。

​	并且向该表中导入MapReduce程序对采集的数据进行[数据预处理后生成的结果文件]()。



#### (1)明细表weblog_dwd

##### a.创建明细表

​	创建明细表[weblog_dwd]()，命令如下。

```hive
create table if not exists
weblog_dw.weblog_dwd(
    visit_ip    string,
    visit_date  string,
    visit_request   string,
    visit_url   string,
    visit_code  string
)
row format delimited
fields terminated by ','
lines terminated by '\n';
```

<img src="./最终章 网站流量日志分系统.assets/image-20231218211430540.png" alt="image-20231218211430540" style="zoom:67%;" />



##### b.明细表导入数据

​	[路径一定要选择自己的！！！]()

​	我们每个人导入的路径都是[不一样]()的，[自己找到MR处理后的数据文件夹（根据日期）]()



​	通过[加载文件]()的方式。

​	将MapReduce程序在[HDFS]()的目录[/output/weblog/你自己的日期](/output/weblog/你自己的日期)中生成的结果文件[part-r-00000]()，导入到明细表[weblog_dwd]()，命令如下。

```hive
load data inpath '/output/weblog/你自己的日期！！！/part-r-00000' 
into table weblog_dw.weblog_dwd;
```

​	由于路径就是hdfs上的，所以不需要参数local

<img src="./最终章 网站流量日志分系统.assets/image-20231218211949378.png" alt="image-20231218211949378" style="zoom:80%;" />

​	

##### c.查询导入数据

​	查询[weblog_dwd]()导入的前5行数据，验证MapReduce程序结果文件中的数据是否导入到明细表weblog_dws

```hive
select * from weblog_dw.weblog_dwd limit 5;
```

<img src="./最终章 网站流量日志分系统.assets/image-20231218212032490.png" alt="image-20231218212032490" style="zoom:67%;" />



#### (2)中间表weblog_dwm

​	在数据库weblog_dw中创建[中间表weblog_dwm]()，并且将明细表weblog_dws中字段[visit_date的数据进行拆分]()处理后的结果[插入]()到中间表weblog_dwm。

##### a.创建中间表

​	创建[中间表weblog_dwm]()，命令如下。

```hive
create table if not exists
weblog_dw.weblog_dwm(
    visit_ip     string,
    visit_split_time     string,
    visit_request      string,
    visit_url         string,
    visit_code        string
) 
partitioned by (visit_split_date string)
row format delimited
fields terminated by ','
lines terminated by '\n';
```

<img src="./最终章 网站流量日志分系统.assets/image-20231218212452276.png" alt="image-20231218212452276" style="zoom: 80%;" />



##### b.开启动态分区

​	中间表开启了根据[visit_split_date]()的分区，因此我们可以通过[动态分区]()的方式，将明细表weblog_dwd的[处理结果导入]()到[中间表weblog_dwm]()。

​	

​	设置动态分区命令如下：

```hive
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
```

<img src="./最终章 网站流量日志分系统.assets/image-20231218212720834.png" alt="image-20231218212720834" />



##### c.插入数据

​	通过[查询插入]()的方式，将明细表weblog_dwd的处理结果插入到中间表weblog_dwd，命令如下。

```hive
insert overwrite table 
     weblog_dw.weblog_dwm 
partition 
     (visit_split_date) 
select 
     visit_ip,split(visit_date,' ')[1],
     visit_request,visit_url,
     visit_code,split(visit_date,' ')[0] 
from 
     weblog_dw.weblog_dwd;
```

<img src="./最终章 网站流量日志分系统.assets/image-20231218213014084.png" alt="image-20231218213014084" style="zoom:67%;" />



##### d.查询导入数据

​	查询中间表[weblog_dwm]()中：

​	[分区]()为visit_split_date=2022-08-08的前5条数据。

​	验证weblog_dws的处理结果是否成功导入到weblog_dwm，命令如下。

```hive
select * from weblog_dw.weblog_dwm where visit_split_date='2022-08-08' limit 5;
```

<img src="./最终章 网站流量日志分系统.assets/image-20231218213244402.png" alt="image-20231218213244402" />



#### (3) 业务表weblog_pv_da

​	在数据仓库weblog_dw创建[业务表weblog_pv_da]()，命令如下。

```hive
create table if not exists
weblog_dw.weblog_pv_da(
     datestr string,
     pv bigint
)
row format delimited
fields terminated by ','
lines terminated by '\n';
```

<img src="./最终章 网站流量日志分系统.assets/image-20231218213433464.png" alt="image-20231218213433464" style="zoom:80%;" />



#### (4)业务表weblog_avguv_da

​	在数据仓库weblog_dw创建[业务表weblog_avguv_da]()，命令如下。

```hive
create table if not exists
weblog_dw.weblog_avguv_da(
    datestr string,
    avguv int
) 
row format delimited
fields terminated by ','
lines terminated by '\n';
```

<img src="./最终章 网站流量日志分系统.assets/image-20231218213554733.png" alt="image-20231218213554733" style="zoom:80%;" />





## 9.6 模块四 数据分析

​	掌握数据分析，能够使用[HiveQL]()实现本系统分析的需求



#### 9.6.1 浏览量分析

​	基于[中间表weblog_dwm]()统计网站的页面[每天]()被用户浏览的次数，并将统计结果插入到[业务表weblog_pv_da]()。

​	思路：

​		对[中间表weblog_dwm]()根据浏览日期（[visit_split_date]()）进行分组，聚合统计（[count]()）总次数

​		并将结果插入到[业务表weblog_pv_da]()

​	命令如下：

```hive
insert into table 
    weblog_dw.weblog_pv_da 
select 
    visit_split_date,
    count(*) 
from 
    weblog_dw.weblog_dwm 
group by 
    visit_split_date;
```

<img src="./最终章 网站流量日志分系统.assets/image-20231219100829763.png" alt="image-20231219100829763" style="zoom: 50%;" />

​	

​	查询分析结果

```hive
select * from weblog_dw.weblog_pv_da;
```



​	拓展问题：如果需要统计[每个页面每天被用户浏览的次数]()，应该如何实现？



#### 9.6.2 人均浏览页数分析

​	基于[中间表weblog_dwm]()统计网站的[页面被每个用户平均浏览的次数]()，并将统计结果插入到[业务表weblog_avguv_da]()，命令如下。

​	思路：

​		对[中间表weblog_dwm]()根据浏览日期（[visit_split_date]()）进行分组。

​		聚合统计（[count]()）总次数，并将结果除以ip数（[distinct visit_ip]()）

​		并将结果插入到[业务表weblog_pv_da]()		

```hive
insert into table 
    weblog_dw.weblog_avguv_da
select 
    visit_split_date,
    count(*)/count(distinct visit_ip) 
from 
    weblog_dw.weblog_dwm 
group by 
    visit_split_date;
```

<img src="./最终章 网站流量日志分系统.assets/image-20231219101027297.png" alt="image-20231219101027297" style="zoom:50%;" />



​	查询分析结果

```hive
select * from weblog_dw.weblog_avguv_da
```



## 9.7 模块五 数据导出

​	掌握数据导出，能够使用[Sqoop完成数据导出]()的操作

​	接下来，使用[Sqoop]()将Hive数据仓库中业务表[weblog_pv_da]()和[weblog_avguv_da]()的数据[导出到MySQL]()的数据表。

### 9.7.1 MySQL创建指定库表

#### (1)登录MySQL

​	在虚拟机master登录MySQL，命令如下。

```shell
mysql -uroot -pPassword123$
```

<img src="./最终章 网站流量日志分系统.assets/image-20231219090930716.png" alt="image-20231219090930716" style="zoom:50%;" />



#### (2)创建数据库

​	在MySQL创建[数据库sqoop_weblog]()，该数据库用于存放本项目相关的数据表，命令如下。

```mysql
create database if not exists sqoop_weblog;
```

<img src="./最终章 网站流量日志分系统.assets/image-20231219091033119.png" alt="image-20231219091033119" style="zoom:67%;" />



#### (3)创建数据表weblog_pv_da

​	在数据库sqoop_weblog中创建[数据表weblog_pv_da]()，该数据表用于[存放]()Hive数据仓库中业务表weblog_pv_da[导出的数据]()，命令如下。

```mysql
CREATE TABLE sqoop_weblog.weblog_pv_da (
   datestr VARCHAR(20),
   pv INT
);
```

<img src="./最终章 网站流量日志分系统.assets/image-20231219101404318.png" alt="image-20231219101404318" style="zoom:80%;" />



#### (4)创建数据表weblog_avguv_da

​	在数据库sqoop_weblog中创建[数据表weblog_avguv_da]()，该数据表用于[存放]()Hive数据仓库中业务表weblog_avguv_da[导出的数据]()，命令如下。

```mysql
CREATE TABLE sqoop_weblog.weblog_avguv_da (
   datestr VARCHAR(20),
   avguv INT
);
```

<img src="./最终章 网站流量日志分系统.assets/image-20231219101427726.png" alt="image-20231219101427726" style="zoom:80%;" />



### 9.7.2 数据导出

#### (1)导出weblog_pv_da的数据

​	[导出]()业务表weblog_pv_da的数据到MySQL的数据表weblog_pv_da，命令如下。

```shell
sqoop export \
--connect jdbc:mysql://master:3306/sqoop_weblog \
--username root \
--password Password123$ \
--columns datestr,pv \
--table weblog_pv_da \
--export-dir /weblog_dw/weblog_pv_da \
--num-mappers 1
```

<img src="./最终章 网站流量日志分系统.assets/image-20231219101525517.png" alt="image-20231219101525517" style="zoom:67%;" />



​	在MySQL中查询导出的数据

```mysql
mysql> select * from sqoop_weblog.weblog_pv_da;
```

<img src="./最终章 网站流量日志分系统.assets/image-20231219101817006.png" alt="image-20231219101817006" style="zoom:67%;" />



#### (2)导出weblog_avguv_da的数据

​	[导出]()业务表weblog_avguv_da的数据到MySQL的数据表weblog_avguv_da，命令如下。

```shell
sqoop export \
--connect jdbc:mysql://master:3306/sqoop_weblog \
--username root \
--password Password123$ \
--columns datestr,avguv  \
--table weblog_avguv_da \
--export-dir /weblog_dw/weblog_avguv_da \
--num-mappers 1
```

<img src="./最终章 网站流量日志分系统.assets/image-20231219102218557.png" alt="image-20231219102218557" style="zoom:80%;" />



​	

​	在MySQL中查询导出的数据

```mysql
mysql> select * from sqoop_weblog.weblog_avguv_da;
```

<img src="./最终章 网站流量日志分系统.assets/image-20231219102358855.png" alt="image-20231219102358855" style="zoom:50%;" />





## 9.8 模块六 数据可视化

​	由于本学期我们还未学习数据可视化（echarts和前端），因此我们用一个可视化工具来测试。

​	本模块我们采用[FineBI]()的方式连接数据库并完成数据可视化。



### 9.8.1 下载并安装FineBI

#### (1)下载FineBI

​	访问FineBI官网，通过注册免费试用的方式下载Windows x64位操作系统的FineBI安装包[windows-x64_FineBI5_1-CN.exe]()。



#### (2)安装FineBI



1. FineBI安装向导界面

   ​	[双击]()FineBI安装包windows-x64_FineBI5_1-CN.exe，进入FineBI安装向导界面

   <img src="./最终章 网站流量日志分系统.assets/image-20231219105235333.png" alt="image-20231219105235333" style="zoom: 33%;" />

2. 欢迎使用FineBI安装程序向导界面

   ​	在FineBI安装向导界面，等待准备工作完成之后，进入欢迎使用FineBI安装程序向导界面。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219105318087.png" alt="image-20231219105318087" style="zoom: 33%;" />

3. 许可协议界面

   进入许可协议界面，在该界面勾选“[我接受协议]()”选项。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219105354590.png" alt="image-20231219105354590" style="zoom: 33%;" />

4. 选择安装目录界面

   ​	在该界面中配置FineBI的[安装目录]()，这里配置FineBI的安装目录位D:\FineBI5.1。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219105431595.png" alt="image-20231219105431595" style="zoom:43%;" />

5. 设置最大内存

   ​	在“最大jvm内存”输入框中输入最大jvm内存，可以根据实际情况去设置，建议设置最大jvm内存至少为[2048]()，设置最大jvm内存为[4096]()。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219105507274.png" alt="image-20231219105507274" style="zoom: 40%;" />

6. 选择开始菜单文件夹

   ​	进入选择开始菜单文件夹界面，在该界面使用[默认配置]()即可。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219105545570.png" alt="image-20231219105545570" style="zoom:33%;" />

7. 选择附加工作

   进入选择附加工作界面，在[该界面使用默认配置]()即可。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219105613373.png" alt="image-20231219105613373" style="zoom: 33%;" />

8. FineBI自动安装

   进入安装中界面，在该界面FineBI会[自动安装]()。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219105643917.png" alt="image-20231219105643917" style="zoom:33%;" />

9. 完成FineBI的安装

   进入完成FineBI安装程序界面，在该界面使用[默认配置]()即可。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219105711649.png" alt="image-20231219105711649" style="zoom:40%;" />

10. 申请激活码

    跳出窗口【激活码窗口】

    <img src="./最终章 网站流量日志分系统.assets/image-20231219111603275.png" alt="image-20231219111603275" style="zoom:67%;" />

    使用激活码

    ```
    99205f31-91737077b-3470-36d51c6fab7f
    ```

    <img src="./最终章 网站流量日志分系统.assets/image-20231219112008881.png" alt="image-20231219112008881" style="zoom:50%;" />

    

    如果激活码失效，点击【点击！获取激活码】，自行注册账号获取

    <img src="./最终章 网站流量日志分系统.assets/image-20231219111708239.png" alt="image-20231219111708239" style="zoom: 33%;" />

    

11. 启动FineBI

    ​	自动运行FineBI。

    <img src="./最终章 网站流量日志分系统.assets/image-20231219105742397.png" alt="image-20231219105742397" style="zoom:33%;" />

12. FineBI平台

    ​	在FineBI启动界面，等待FineBI启动完成后，系统默认配置的[浏览器会打开FineBI平台]()。

    <img src="./最终章 网站流量日志分系统.assets/image-20231219105819950.png" alt="image-20231219105819950" style="zoom:43%;" />

    

### 9.8.2 配置FineBI登录用户

FineBI地址：http://localhost:37799/webroot/decision/login/initialization

1. 设置管理员账号

   ​	首次使用FineBI时需要通过FineBI平台进行[初始化]()设置。

   ​	设置管理员账号：

   ​		将用户名设置为[lcvc]()

   ​		密码设置为[123456]()

   <img src="./最终章 网站流量日志分系统.assets/image-20231219112154093.png" alt="image-20231219112154093" style="zoom: 50%;" />

   

2. 管理员账号设置成功

   ​	管理员账号设置成功后会显示“[管理员账号设置成功]()”信息。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219112259236.png" alt="image-20231219112259236" style="zoom:50%;" />

3. 选择FineBI使用的数据库

   暂时不配置FineBI使用的数据库，选择[直接登录]()即可。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219112402218.png" alt="image-20231219112402218" style="zoom:50%;" />

4. 登录FineBI

   在FineBI登录界面输入用户名和密码，[登录FineBI]()。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219112828273.png" alt="image-20231219112828273" style="zoom: 50%;" />

5. 成功登录FineBI平台

   在FineBI登录界面设置用户名和密码，单击“登录”按钮登录FineBI。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219113025337.png" alt="image-20231219113025337" style="zoom: 33%;" />

   

### 9.8.3 配置FineBI数据连接

​	数据连接用于配置FineBI使用的数据库，本项目[通过FineBI获取虚拟机master中MySQL的数据进行可视化展示]()。

1. 数据连接管理

   ​	依次单击FineBI的[“管理系统”→“数据连接”→“数据连接管理”]()按钮进入数据连接管理界面。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219113208886.png" alt="image-20231219113208886" style="zoom:50%;" />

2. 选择FineBI使用的数据库

   选择FineBI使用的数据库为[MySQL]()。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219113532501.png" alt="image-20231219113532501" style="zoom: 50%;" />

3. 配置MySQL连接信息

   ​	[配置连接MySQL的相关信息]()，包括数据连接名称、数据库名称、端口、主机、用户名和密码。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219114311887.png" alt="image-20231219114311887" style="zoom:50%;" />

4. 测试连接

   验证FineBI是否可以[成功连接虚拟机master的MySQL]()。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219114324979.png" alt="image-20231219114324979" style="zoom:50%;" />



### 9.8.4 实现数据可视化

​	掌握数据可视化，能够使用[FineBI对分析结果数据进行可视化展示]()

#### (1)读取数据

1. 数据准备

   在FineBI商业智能平台，选择FineBI[“数据准备”]()选项。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219114553092.png" alt="image-20231219114553092" style="zoom:50%;" />

2. 配置业务名称

   ​	在FineBI商业智能平台[添加业务包]()，并配置业务名称为“[网站流量日志数据分析系统]()” 

   <img src="./最终章 网站流量日志分系统.assets/image-20231219114629393.png" alt="image-20231219114629393" style="zoom:50%;" />

3. 配置业务使用的数据内容

   ​	单击“网站流量日志数据分析系统”，[配置该业务使用]()的数据内容。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219114700853.png" alt="image-20231219114700853" style="zoom:67%;" />

   ​	单机【添加表】，选择【数据库表】

   <img src="./最终章 网站流量日志分系统.assets/image-20231219114947963.png" alt="image-20231219114947963" style="zoom:67%;" />

   ​	

   ​	勾选上数据表[weblog_avguv_da]()和[weblog_pv_da]()，并点击【确定】

   <img src="./最终章 网站流量日志分系统.assets/image-20231219115032330.png" alt="image-20231219115032330" style="zoom:67%;" />

4. 业务包更新

   ​	更新业务包“[网站流量日志数据分析系统]()”中的数据表weblog_avguv_da和weblog_pv_da中的数据，使数据生效。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219115224005.png" alt="image-20231219115224005" style="zoom: 50%;" />

   <img src="./最终章 网站流量日志分系统.assets/image-20231219115301484.png" alt="image-20231219115301484" style="zoom: 33%;" />

   ​	更新后，数据就被加载到FineBI中了。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219115337640.png" alt="image-20231219115337640" />



#### (2) 实现浏览量分析结果的可视化

1. 新建仪表板

   ​	在FineBI商业智能平台，单击FineBI的[“仪表版”]()选项进入仪表板管理页面，在该页面单击[“新建仪表板”]()按钮新建名称为[“网站流量日志数据分析系统”]()。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219115839659.png" alt="image-20231219115839659" style="zoom:67%;" />

2. 配置仪表板

   ​	[网站流量日志数据分析系统]()的仪表板创建完之后，此时会跳转到该仪表板的配置界面。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219115911408.png" alt="image-20231219115911408" style="zoom:50%;" />

3. 添加组件

   ​	在仪表板界面，单击[“添加组件”]()按钮，进入到添加组件页面。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219115936302.png" alt="image-20231219115936302" style="zoom: 67%;" />

4. 选择使用的数据表

   选择组件使用数据库master中的[数据表weblog_pv_da]()。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219120904997.png" alt="image-20231219120904997" style="zoom: 50%;" />

5. 配置组件

   组件的数据表配置完成后进入到[组件的配置界面]()。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219120443057.png" alt="image-20231219120443057" style="zoom:67%;" />

6. 配置组件名称

   配置组件的名称为[浏览量分析]()。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219120503735.png" alt="image-20231219120503735" style="zoom:50%;" />

7. 选择图标类型

   选择组件的图标类型为[“分区折线图”]() 。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219120529211.png" alt="image-20231219120529211" style="zoom:67%;" />

8. 配置分区折线图

   ​	将维度一栏中的[“datestr”]()拖入[“横轴”]()，然后将指标一栏中的[“pv”]()拖入[“纵轴”]()和图形属性中的[“标签”]() 。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219120620108.png" alt="image-20231219120620108" style="zoom:80%;" />

9. 实现浏览量分析结果可视化

   ​	在预览仪表板中查看组件[“浏览量分析”]() 。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219120713153.png" alt="image-20231219120713153" />



#### (3)实现人均浏览页数分析结果的可视化

1. 添加组件

   ​	添加新的可视化组件

   <img src="./最终章 网站流量日志分系统.assets/image-20231219121213293.png" alt="image-20231219121213293" />

   

2. 配置组件使用的数据表

   ​	选择组件使用数据库master中的[数据表weblog_avguv_da]()。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219121243040.png" alt="image-20231219121243040" style="zoom:67%;" />

3. 配置组件

   ​	组件的数据表配置完成后进入到组件的[配置界面]()。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219121338731.png" alt="image-20231219121338731" />

4. 配置组件名称

   ​	配置组件的名称为[人均浏览页数分析]()。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219121356055.png" alt="image-20231219121356055" style="zoom: 50%;" />

5. 选择图标类型

   ​	选择图标类型为[“多系列柱状图”]() 。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219121423382.png" alt="image-20231219121423382" style="zoom:67%;" />

6. 配置多系列柱状图

   ​	将维度一栏中的[“datestr”]()拖入[“横轴”]()，然后将指标一栏中的[“avguv”]()拖入[“纵轴”]()和图形属性中的[“标签”]() 。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219121454332.png" alt="image-20231219121454332" style="zoom:80%;" />

7. 实现人均访问页数分析结果可视化

   ​	在预览仪表板中查看组件实现“网站流量日志数据分析系统”的效果。

   <img src="./最终章 网站流量日志分系统.assets/image-20231219121516988.png" alt="image-20231219121516988" style="zoom:80%;" />

   



## 小结

​	本章通过开发网站流量日志数据分析系统，讲解如何利用Hadoop生态体系的技术解决实际问题。

​	首先介绍系统的背景、需求和架构，然后逐个讲解各个模块之间的实现方式，包括数据采集、数据预处理、数据仓库开发、数据分析、数据导出以及数据可视化。

​	读者需要掌握系统架构以及数据处理流程，熟练使用Hadoop生态体系相关技术，完成系统中各个模块的开发，这样才能将本书Hadoop知识体系融会贯通。
